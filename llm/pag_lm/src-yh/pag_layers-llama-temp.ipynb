{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit PAG\n",
    "\n",
    "GenerationMixin.generate 이 좀 어질어질하네.\n",
    "\n",
    "되도록 건드리지 않는 것을 추구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/nsml/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "token = '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'\n",
    "huggingface_hub.login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 12.87s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 45s, sys: 45.5 s, total: 5min 30s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.float16}, device_map=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hook\n",
    "from pag.utils import get_module_by_name\n",
    "\n",
    "# Adapted from LlamaAttention.forward\n",
    "def perturb_attn_map(module, input, kwargs, output, inv_temp: float=1.0, guidance_scale: float=7.5):\n",
    "    hidden_states = kwargs['hidden_states']\n",
    "    attention_mask = kwargs['attention_mask']\n",
    "    position_ids = kwargs['position_ids']\n",
    "    past_key_value = kwargs['past_key_value']\n",
    "    output_attentions = kwargs['output_attentions']\n",
    "    use_cache = kwargs['use_cache']\n",
    "    cache_position = kwargs['cache_position']\n",
    "\n",
    "    if output_attentions:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = module.q_proj(hidden_states)\n",
    "    key_states = module.k_proj(hidden_states)\n",
    "    value_states = module.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, module.num_heads, module.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, module.num_key_value_heads, module.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, module.num_key_value_heads, module.head_dim).transpose(1, 2)\n",
    "\n",
    "    cos, sin = module.rotary_emb(value_states, position_ids)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    # In case static cache is used, it is an instance attribute.\n",
    "    past_key_value = getattr(module, \"past_key_value\", past_key_value)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, module.layer_idx, cache_kwargs)\n",
    "\n",
    "    key_states = repeat_kv(key_states, module.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, module.num_key_value_groups)\n",
    "\n",
    "    # original attn\n",
    "    k_len = key_states.size(-2)\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, :k_len]\n",
    "\n",
    "    if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "    attn_output_o = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=module.attention_dropout if module.training else 0.0,\n",
    "        is_causal=causal_mask is None and q_len > 1,\n",
    "    )\n",
    "\n",
    "    attn_output_o = attn_output_o.transpose(1, 2).contiguous()\n",
    "    attn_output_o = attn_output_o.view(bsz, q_len, module.hidden_size)\n",
    "\n",
    "    attn_output_o = module.o_proj(attn_output_o)\n",
    "\n",
    "    # perturbed attn\n",
    "    if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "    attn_output_p = torch.nn.functional.scaled_dot_product_attention(\n",
    "        inv_temp * query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=module.attention_dropout if module.training else 0.0,\n",
    "        is_causal=causal_mask is None and q_len > 1,\n",
    "    )\n",
    "\n",
    "    attn_output_p = attn_output_p.transpose(1, 2).contiguous()\n",
    "    attn_output_p = attn_output_p.view(bsz, q_len, module.hidden_size)\n",
    "\n",
    "    attn_output_p = module.o_proj(attn_output_p)\n",
    "\n",
    "    # guidance\n",
    "    attn_output = attn_output_o + guidance_scale * (attn_output_o - attn_output_p)\n",
    "\n",
    "    return attn_output, None, past_key_value\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval\n",
    "\n",
    "MMLU : 5-shot\n",
    "\n",
    "HumanEval : zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"Idavidrein/gpqa\", \"gpqa_diamond\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "Example = namedtuple('Example', ['question', 'choice1', 'choice2', 'choice3', 'choice4', 'correct_index'])\n",
    "\n",
    "def shuffle_choices_and_create_example(row) -> Example:\n",
    "    list_choices = [row['Incorrect Answer 1'], row['Incorrect Answer 2'], row['Incorrect Answer 3'], row['Correct Answer']]\n",
    "    random.shuffle(list_choices)\n",
    "    example = Example(row['Question'], list_choices[0], list_choices[1], list_choices[2], list_choices[3], list_choices.index(row['Correct Answer']))\n",
    "    return example\n",
    "\n",
    "example_list = [shuffle_choices_and_create_example(row) for row in ds['train']]\n",
    "\n",
    "def zero_shot_prompt(example) -> str:\n",
    "    \"\"\"Creates a zero-shot prompt given a single example. Uses the prompt format from this paper on Scalable Oversight: \n",
    "    https://arxiv.org/abs/2211.03540\"\"\"\n",
    "    prompt = base_prompt(example)\n",
    "    prompt += f\"\\n\\nFormat your response as follows: \\\"The correct answer is (insert answer here)\\\"\"\n",
    "    return prompt\n",
    "\n",
    "def base_prompt(example) -> str:\n",
    "    \"\"\"Creates a zero-shot prompt given a single example. Uses the prompt format from this paper on Scalable Oversight: \n",
    "    https://arxiv.org/abs/2211.03540\"\"\"\n",
    "    prompt = f\"What is the correct answer to this question: {example.question}\"\n",
    "    prompt += f\"\\n\\nChoices:\\n(A) {example.choice1}\\n(B) {example.choice2}\\n(C) {example.choice3}\\n(D) {example.choice4}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: (B)\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 71\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handle \u001b[38;5;129;01min\u001b[39;00m handles:\n\u001b[1;32m     69\u001b[0m     handle\u001b[38;5;241m.\u001b[39mremove()\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# print()\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# print(f'### guidance_scale_{round(guidance_scale, 2)}-inv_temp_{inv_temp} ###')\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# print(outputs[0]['generated_text'])\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# print()\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llama_layer_list = [\n",
    "    # 'model.layers.0.self_attn',\n",
    "    # 'model.layers.1.self_attn',\n",
    "    # 'model.layers.2.self_attn',\n",
    "    # 'model.layers.3.self_attn',\n",
    "    # 'model.layers.4.self_attn',\n",
    "    # 'model.layers.5.self_attn',\n",
    "    # 'model.layers.6.self_attn',\n",
    "    'model.layers.7.self_attn',\n",
    "    'model.layers.8.self_attn',\n",
    "    'model.layers.9.self_attn',\n",
    "    'model.layers.10.self_attn',\n",
    "    'model.layers.11.self_attn',\n",
    "    'model.layers.12.self_attn',\n",
    "    'model.layers.13.self_attn',\n",
    "    'model.layers.14.self_attn',\n",
    "    'model.layers.15.self_attn',\n",
    "    # 'model.layers.16.self_attn',\n",
    "    # 'model.layers.17.self_attn',\n",
    "    # 'model.layers.18.self_attn',\n",
    "    # 'model.layers.19.self_attn',\n",
    "    # 'model.layers.20.self_attn',\n",
    "    # 'model.layers.21.self_attn',\n",
    "    # 'model.layers.22.self_attn',\n",
    "    # 'model.layers.23.self_attn',\n",
    "    # 'model.layers.24.self_attn',\n",
    "    # 'model.layers.25.self_attn',\n",
    "    # 'model.layers.26.self_attn',\n",
    "    # 'model.layers.27.self_attn',\n",
    "    # 'model.layers.28.self_attn',\n",
    "    # 'model.layers.29.self_attn',\n",
    "    # 'model.layers.30.self_attn',\n",
    "    # 'model.layers.31.self_attn',\n",
    "]\n",
    "\n",
    "for example in example_list:\n",
    "    prompt = base_prompt(example)\n",
    "    break\n",
    "\n",
    "max_length = 256\n",
    "choices = ['(A)', '(B)', '(C)', '(D)']\n",
    "choices_logit_idx_list = []\n",
    "for choice in choices:\n",
    "    logit_idx = pipe.tokenizer.encode(choice)\n",
    "    assert len(logit_idx) == 3\n",
    "    choices_logit_idx_list.append(logit_idx[1])\n",
    "choices_logit_idx_tensor = torch.tensor(choices_logit_idx_list, device='cuda')\n",
    "\n",
    "print('answer: {}'.format(choices[example.correct_index]))\n",
    "inputs = pipe.tokenizer([prompt], return_tensors=\"pt\")\n",
    "inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "# outputs = pipe(prompt, max_length=max_length, truncation=True)\n",
    "print()\n",
    "# print(f'### no guidance ###')\n",
    "# print(outputs[0]['generated_text'])\n",
    "print()\n",
    "        \n",
    "guidance_scale_list = [0.25, 0.5, 0.75, 1.0]\n",
    "for guidance_scale in guidance_scale_list:\n",
    "    for inv_temp in [1/1e1, 1/1e2]:\n",
    "        # hook (make attn map identity)\n",
    "        handles = []\n",
    "        for layer_name in hook_layer_list:\n",
    "            module = get_module_by_name(pipe.model, layer_name)\n",
    "            perturb_attn_map_fn = partial(perturb_attn_map, guidance_scale=guidance_scale, inv_temp=inv_temp)\n",
    "            handle = module.register_forward_hook(perturb_attn_map_fn, with_kwargs=True)\n",
    "            handles.append(handle)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = pipe.model(**inputs)\n",
    "\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "            \n",
    "        correct += (\n",
    "            output.logits[0, -1, choices_logit_idx_tensor].argmax().item()\n",
    "            == example.correct_index\n",
    "        )\n",
    "\n",
    "        # print()\n",
    "        # print(f'### guidance_scale_{round(guidance_scale, 2)}-inv_temp_{inv_temp} ###')\n",
    "        # print(outputs[0]['generated_text'])\n",
    "        # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = (\n",
    "    'guidance_scale_{}-temp_{}'\n",
    "    .format(guidance_scale, round(1/inv_temp, 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4444, 5462, 3100, 5549]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices_logit_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128256])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits[0, -1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
