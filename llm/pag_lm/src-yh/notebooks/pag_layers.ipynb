{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# GPT-2 모델과 토크나이저 로드\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 수정할 layer 번호 설정\n",
    "modify_layer = 2\n",
    "\n",
    "# Attention map을 identity matrix로 설정하는 함수\n",
    "def modify_attention(module, input, output):\n",
    "    if isinstance(module, torch.nn.MultiheadAttention):\n",
    "        layer_num = int(module.layer_num)\n",
    "        if layer_num == modify_layer:\n",
    "            attention_probs = output[1]\n",
    "            batch_size, num_heads, seq_length, _ = attention_probs.shape\n",
    "            attention_probs.data = torch.eye(seq_length).expand(batch_size, num_heads, -1, -1)\n",
    "    return output\n",
    "\n",
    "# Hook 등록 함수\n",
    "def register_hooks(model):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.MultiheadAttention):\n",
    "            layer_num = name.split(\".\")[2]\n",
    "            module.layer_num = layer_num\n",
    "            module.register_forward_hook(modify_attention)\n",
    "\n",
    "# Hook 등록\n",
    "register_hooks(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성 함수\n",
    "def generate_text(input_text, guidance_scale):\n",
    "    # 입력 텍스트 인코딩\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    # Attention map 수정 후 forward 계산\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_state_p = outputs.hidden_states[modify_layer]\n",
    "    \n",
    "    # 원래 attention으로 forward 계산\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_state_o = outputs.hidden_states[modify_layer]\n",
    "    \n",
    "    # Hidden state 계산\n",
    "    hidden_state = hidden_state_p + guidance_scale * (hidden_state_p - hidden_state_o)\n",
    "    \n",
    "    # 텍스트 생성\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=attention_mask,\n",
    "        logits_processor=[\n",
    "            lambda input_ids, scores: scores + guidance_scale * (scores - model(inputs_embeds=hidden_state).logits[:, -1, :])\n",
    "        ],\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "Hello, how are you? I'm doing well. Thank goodness.\" ―Wilson Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce Pearce\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 생성 및 출력\n",
    "input_text = \"Hello, how are you? I'm doing well.\"\n",
    "guidance_scale = 6.0\n",
    "generated_text = generate_text(input_text, guidance_scale)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 계산 함수\n",
    "def calculate_perplexity(model, tokenizer, text):\n",
    "    encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_text, labels=encoded_text)\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss)\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3.57\n"
     ]
    }
   ],
   "source": [
    "# Perplexity 계산 및 출력\n",
    "perplexity = calculate_perplexity(model, tokenizer, generated_text)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
