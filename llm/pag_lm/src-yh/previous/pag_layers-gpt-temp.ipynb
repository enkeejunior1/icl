{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit PAG\n",
    "\n",
    "GenerationMixin.generate 이 좀 어질어질하네.\n",
    "\n",
    "되도록 건드리지 않는 것을 추구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# define hook\n",
    "from pag.utils import get_module_by_name\n",
    "\n",
    "def perturb_attn_map(module, input, kwargs, output, inv_temp:float = 1.0, guidance_scale:float = 7.5):\n",
    "    layer_past = kwargs['layer_past']\n",
    "    use_cache = kwargs['use_cache']\n",
    "    head_mask = kwargs['head_mask']\n",
    "    attention_mask = kwargs['attention_mask']\n",
    "    output_attentions = kwargs['output_attentions']\n",
    "\n",
    "    hidden_states = input[0]\n",
    "    query, key, value = module.c_attn(hidden_states).split(module.split_size, dim=2)\n",
    "\n",
    "    query = module._split_heads(query, module.num_heads, module.head_dim)\n",
    "    key = module._split_heads(key, module.num_heads, module.head_dim)\n",
    "    value = module._split_heads(value, module.num_heads, module.head_dim)\n",
    "\n",
    "    if layer_past is not None:\n",
    "        past_key, past_value = layer_past\n",
    "        key = torch.cat((past_key, key), dim=-2)\n",
    "        value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "    if use_cache is True:\n",
    "        present = (key, value)\n",
    "    else:\n",
    "        present = None\n",
    "\n",
    "    # original attn \n",
    "    if module.reorder_and_upcast_attn:\n",
    "        attn_output_o, attn_weights_o = module._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n",
    "    else:\n",
    "        attn_output_o, attn_weights_o = module._attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "    attn_output_o = module._merge_heads(attn_output_o, module.num_heads, module.head_dim)\n",
    "    attn_output_o = module.c_proj(attn_output_o)\n",
    "    attn_output_o = module.resid_dropout(attn_output_o)\n",
    "    \n",
    "    # perturbed attn\n",
    "    if module.reorder_and_upcast_attn:\n",
    "        attn_output_p, attn_weights_p = module._upcast_and_reordered_attn(inv_temp * query, key, value, attention_mask, head_mask)\n",
    "    else:\n",
    "        attn_output_p, attn_weights_p = module._attn(inv_temp * query, key, value, attention_mask, head_mask)\n",
    "\n",
    "    attn_output_p = module._merge_heads(attn_output_p, module.num_heads, module.head_dim)\n",
    "    attn_output_p = module.c_proj(attn_output_p)\n",
    "    attn_output_p = module.resid_dropout(attn_output_p)\n",
    "\n",
    "    # guidance\n",
    "    attn_output = attn_output_o + guidance_scale * (attn_output_o - attn_output_p)\n",
    "    \n",
    "    outputs = (attn_output, present)\n",
    "    if output_attentions:\n",
    "        outputs += (attn_weights_o,)\n",
    "\n",
    "    return outputs  # a, present, (attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsml/.local/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for hails/mmlu_no_train contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hails/mmlu_no_train\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading data: 100%|██████████| 166M/166M [00:09<00:00, 17.2MB/s] \n",
      "Generating test split: 126 examples [00:00, 770.29 examples/s]\n",
      "Generating validation split: 14 examples [00:00, 2296.27 examples/s]\n",
      "Generating dev split: 5 examples [00:00, 36.29 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset('hails/mmlu_no_train', 'formal_logic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text: guidance_scale 0.0-inv_temp_1.0###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The following is a list of the most common mistakes that people make when translating a predicate into a predicate logic.\n",
      "\n",
      "The following is a list of the most common mistakes that people make when translating a predicate into a predicate logic.\n",
      "\n",
      "The following is a list of the most common mistakes that people make when\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text: guidance_scale 0.3333333333333333-inv_temp_1.0###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The following is a list of the most common mistakes that people make when translating a predicate into a predicate logic.\n",
      "\n",
      "The following is a list of the most common mistakes that people make when translating a predicate into a predicate logic.\n",
      "\n",
      "The following is a list of the most common mistakes that people make when\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text: guidance_scale 0.3333333333333333-inv_temp_0.1###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The first step is to find the predicate logic that will be used to determine the predicate.\n",
      "\n",
      "The first step is to find the predicate logic that will be used to determine the predicate. The first step is to find the predicate logic that will be used to determine the predicate. The first step is to find\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text: guidance_scale 0.3333333333333333-inv_temp_0.01###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The first step is to find the predicate logic that will be used to determine the predicate. The predicate logic is a set of functions that are called \"predicates\" and are used to determine the predicate. The predicate logic is a set of functions that are called \"predicates\" and are used to determine the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text: guidance_scale 0.6666666666666666-inv_temp_1.0###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The following is a list of the most common mistakes that people make when translating a predicate into a predicate logic.\n",
      "\n",
      "The following is a list of the most common mistakes that people make when translating a predicate into a predicate logic.\n",
      "\n",
      "The following is a list of the most common mistakes that people make when\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text: guidance_scale 0.6666666666666666-inv_temp_0.1###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The first step is to define the predicate logic.\n",
      "\n",
      "The predicate logic is a function that takes a predicate and returns a list of values.\n",
      "\n",
      "The predicate logic is a function that takes a predicate and returns a list of values. The predicate logic is a function that takes a predicate and returns a list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text: guidance_scale 0.6666666666666666-inv_temp_0.01###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The first step is to define the predicate logic.\n",
      "\n",
      "The predicate logic is a function that takes a predicate and returns a list of values.\n",
      "\n",
      "The predicate logic is a function that takes a predicate and returns a list of values. The predicate logic is a function that takes a predicate and returns a list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text: guidance_scale 1.0-inv_temp_1.0###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The following is a list of the most common mistakes that people make when translating a predicate into a predicate logic.\n",
      "\n",
      "The following is a list of the most common mistakes that people make when translating a predicate into a predicate logic.\n",
      "\n",
      "The following is a list of the most common mistakes that people make when\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text: guidance_scale 1.0-inv_temp_0.1###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The first thing to do is to make sure that the first argument is a predicate.\n",
      "\n",
      "defmodule foo.foo do |x| x = x.x | x.x = x.x defmodule bar.bar do |x| x.x = x.x defmodule foo.bar\n",
      "\n",
      "### Generated text: guidance_scale 1.0-inv_temp_0.01###\n",
      "Select the best translation into predicate logic. David teaches Chris. (c: Chris; d: David; Txy: x teaches y)\n",
      "\n",
      "The first thing to do is to make sure that the first argument is a predicate.\n",
      "\n",
      "defmodule foo.foo do |x| x.x = x.x defmodule bar.bar do |x| x.x = x.x defmodule foo.bar do |x| x.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "hook_layer_list = [\n",
    "    # 'transformer.h.0.attn', \n",
    "    # 'transformer.h.1.attn', \n",
    "    'transformer.h.2.attn', \n",
    "    'transformer.h.3.attn', \n",
    "    'transformer.h.4.attn', \n",
    "    'transformer.h.5.attn', \n",
    "    # 'transformer.h.6.attn', \n",
    "    # 'transformer.h.7.attn', \n",
    "    # 'transformer.h.8.attn', \n",
    "    # 'transformer.h.9.attn', \n",
    "    # 'transformer.h.10.attn', \n",
    "    # 'transformer.h.11.attn', \n",
    "]\n",
    "\n",
    "prompt = ds['test'][1]['question']\n",
    "\n",
    "for guidance_scale in np.linspace(0, 1, 4):\n",
    "    for inv_temp in [1.0, 1e-1, 1e-2]:\n",
    "        # hook (make attn map identity)\n",
    "        handles = []\n",
    "        for layer_name in hook_layer_list:\n",
    "            module = get_module_by_name(model, layer_name)\n",
    "            perturb_attn_map_fn = partial(perturb_attn_map, inv_temp=inv_temp, guidance_scale=guidance_scale)\n",
    "            handle = module.register_forward_hook(perturb_attn_map_fn, with_kwargs=True)\n",
    "            handles.append(handle)\n",
    "\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=64, return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "        # generated result\n",
    "        print()\n",
    "        print(f\"### Generated text: guidance_scale {guidance_scale}-inv_temp_{inv_temp}###\")\n",
    "        print(f'{tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)}')\n",
    "\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "            \n",
    "        if guidance_scale == 0.0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Today is the day when we can all be proud of our country and our values.\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "print()\n",
    "print(f'{tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Today is the day when we can all be proud of our country and our values.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'layer_past': None, 'attention_mask': torch.tensor([[[[-0., -0.]]]]), 'head_mask': None, 'use_cache': True, 'output_attentions': False}\n",
    "\n",
    "for k, v in kwargs.items():\n",
    "    locals()[k] = v\n",
    "\n",
    "print(layer_past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 3, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> tensor1 = torch.randn(10, 16, 3, 4)\n",
    ">>> tensor2 = torch.randn(10, 16, 4, 5)\n",
    ">>> torch.matmul(tensor1, tensor2).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(guidance_traj[4].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attention map 수정을 위한 모델 복사\n",
    "model_modified = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Attention map을 수정할 layer 번호 설정\n",
    "modify_layers = [2, 4]\n",
    "\n",
    "# Attention map을 identity matrix로 설정하는 함수\n",
    "def modify_attention(module, input, output):\n",
    "    # module이 MultiheadAttention인 경우에만 수정\n",
    "    if isinstance(module, torch.nn.MultiheadAttention):\n",
    "        # layer 번호 가져오기\n",
    "        layer_num = int(module.layer_num)\n",
    "        # 수정할 layer인 경우\n",
    "        if layer_num in modify_layers:\n",
    "            attention_probs = output[1]\n",
    "            batch_size, num_heads, seq_length, _ = attention_probs.shape\n",
    "            print(f\"Layer {layer_num} - Attention shape: {attention_probs.shape}\")\n",
    "            # Attention shape 확인\n",
    "            assert attention_probs.shape[-1] == seq_length, f\"Attention shape mismatch: {attention_probs.shape}\"\n",
    "            # Attention map을 identity matrix로 설정\n",
    "            attention_probs.data = torch.eye(seq_length).expand(batch_size, num_heads, -1, -1)\n",
    "    return output\n",
    "\n",
    "# Hook 등록 함수\n",
    "def register_hooks(model):\n",
    "    # 모든 module에 대해 iterate\n",
    "    for name, module in model.named_modules():\n",
    "        # module이 MultiheadAttention인 경우 hook 등록\n",
    "        if isinstance(module, torch.nn.MultiheadAttention):\n",
    "            layer_num = name.split(\".\")[2]\n",
    "            module.layer_num = layer_num\n",
    "            module.register_forward_hook(modify_attention)\n",
    "\n",
    "# 수정된 모델에 hook 등록\n",
    "register_hooks(model_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성 함수\n",
    "def generate_text(input_text, guidance_scale):\n",
    "    # 입력 텍스트 인코딩\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    # Attention map 수정 후 logit_p 계산\n",
    "    with torch.no_grad():\n",
    "        outputs = model_modified(input_ids, attention_mask=attention_mask)\n",
    "        logit_p = outputs.logits\n",
    "        \n",
    "    # 원래 attention으로 logit_o 계산\n",
    "    with torch.no_grad():\n",
    "        outputs = model_original(input_ids, attention_mask=attention_mask)\n",
    "        logit_o = outputs.logits\n",
    "        \n",
    "    # logit 계산\n",
    "    logit = logit_o + guidance_scale * (logit_o - logit_p)\n",
    "    print(f\"logit shape: {logit.shape}\")\n",
    "    print(f\"logit: {logit}\")\n",
    "    \n",
    "    # 텍스트 생성\n",
    "    output = model_original.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=attention_mask,\n",
    "        logits_processor=[\n",
    "            lambda input_ids, scores: scores + guidance_scale * (scores - logit[:, -1, :])\n",
    "        ],\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated text: {tokenizer.decode(output.sequences[0], skip_special_tokens=True)}\")\n",
    "    print(f\"Processed logits: {output.scores[0].shape}\")\n",
    "    print(f\"Processed logits: {output.scores[0]}\")\n",
    "    \n",
    "    return tokenizer.decode(output.sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit shape: torch.Size([1, 11, 50257])\n",
      "logit: tensor([[[ -35.2362,  -35.3266,  -38.9754,  ...,  -44.4645,  -43.9975,\n",
      "           -36.4580],\n",
      "         [-112.6171, -114.5832, -116.5724,  ..., -119.0128, -118.8059,\n",
      "          -111.6917],\n",
      "         [-116.7137, -117.5931, -123.1624,  ..., -125.6588, -125.2527,\n",
      "          -119.3150],\n",
      "         ...,\n",
      "         [-107.5247, -109.3616, -113.5464,  ..., -115.4737, -118.1396,\n",
      "          -112.0031],\n",
      "         [ -82.1815,  -84.9542,  -93.1059,  ...,  -98.1118,  -98.1967,\n",
      "           -88.8256],\n",
      "         [-147.8117, -146.8510, -149.7308,  ..., -160.3841, -160.5273,\n",
      "          -143.6174]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Hello, how are you? I'm doing well. I intend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend\n",
      "Processed logits: torch.Size([1, 50257])\n",
      "Processed logits: tensor([[-147.8117, -146.8510, -149.7308,  ..., -160.3841, -160.5273,\n",
      "         -143.6174]])\n",
      "Generated Text:\n",
      "Hello, how are you? I'm doing well. I intend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 생성 및 출력\n",
    "input_text = \"Hello, how are you? I'm doing well.\"\n",
    "guidance_scale = 6.0\n",
    "generated_text = generate_text(input_text, guidance_scale)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 계산 함수\n",
    "def calculate_perplexity(model_original, model_modified, tokenizer, text, guidance_scale):\n",
    "    encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "    attention_mask = torch.ones_like(encoded_text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_original = model_original(encoded_text, attention_mask=attention_mask, labels=encoded_text)\n",
    "        loss_original = outputs_original.loss\n",
    "        \n",
    "        outputs_modified = model_modified(encoded_text, attention_mask=attention_mask)\n",
    "        logits_modified = outputs_modified.logits\n",
    "        \n",
    "        logits = outputs_original.logits + guidance_scale * (outputs_original.logits - logits_modified)\n",
    "        loss_modified = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), encoded_text.view(-1))\n",
    "        \n",
    "        perplexity = torch.exp((loss_original + loss_modified) / 2)\n",
    "    \n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 535.28\n"
     ]
    }
   ],
   "source": [
    "# Perplexity 계산 및 출력\n",
    "perplexity = calculate_perplexity(model_original, model_modified, tokenizer, generated_text, guidance_scale)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
