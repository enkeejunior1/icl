{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit PAG\n",
    "\n",
    "GenerationMixin.generate 이 좀 어질어질하네.\n",
    "\n",
    "되도록 건드리지 않는 것을 추구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:48<00:00, 12.24s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.float16}, device_map=\"cuda\"\n",
    ")\n",
    "# pipe(\"Hey how are you doing today?\", max_length=256, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from LlamaAttention.forward\n",
    "def perturb_attn_map(module, input, kwargs, output, guidance_scale: float=7.5):\n",
    "    hidden_states = kwargs['hidden_states']\n",
    "    attention_mask = kwargs['attention_mask']\n",
    "    position_ids = kwargs['position_ids']\n",
    "    past_key_value = kwargs['past_key_value']\n",
    "    output_attentions = kwargs['output_attentions']\n",
    "    use_cache = kwargs['use_cache']\n",
    "    cache_position = kwargs['cache_position']\n",
    "\n",
    "    if output_attentions:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = module.q_proj(hidden_states)\n",
    "    key_states = module.k_proj(hidden_states)\n",
    "    value_states = module.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, module.num_heads, module.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, module.num_key_value_heads, module.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, module.num_key_value_heads, module.head_dim).transpose(1, 2)\n",
    "\n",
    "    cos, sin = module.rotary_emb(value_states, position_ids)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    # In case static cache is used, it is an instance attribute.\n",
    "    past_key_value = getattr(module, \"past_key_value\", past_key_value)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, module.layer_idx, cache_kwargs)\n",
    "\n",
    "    key_states = repeat_kv(key_states, module.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, module.num_key_value_groups)\n",
    "\n",
    "    k_len = key_states.size(-2)\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, :k_len]\n",
    "\n",
    "    # perturbed attn\n",
    "    attention_mask = torch.zeros(k_len, k_len) + torch.eye(k_len)\n",
    "    attention_mask = attention_mask.bool().to(query_states.device)\n",
    "    attention_mask = attention_mask[None, None, :q_len, :k_len] # |b, head, l, l|\n",
    "\n",
    "    if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=attention_mask,\n",
    "        dropout_p=module.attention_dropout if module.training else 0.0,\n",
    "        is_causal=causal_mask is None and q_len > 1,\n",
    "    )\n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.view(bsz, q_len, module.hidden_size)\n",
    "\n",
    "    attn_output = module.o_proj(attn_output)\n",
    "\n",
    "    return attn_output, None, past_key_value\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pag.utils import get_module_by_name\n",
    "\n",
    "# monkey patch forward path\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    CausalLMOutputWithPast,\n",
    ")\n",
    "\n",
    "def forward(\n",
    "    self,\n",
    "    input_ids: torch.LongTensor = None,\n",
    "    attention_mask: Optional[torch.Tensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    "    cache_position: Optional[torch.LongTensor] = None,\n",
    ") -> Union[Tuple, CausalLMOutputWithPast]:\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import AutoTokenizer, LlamaForCausalLM\n",
    "\n",
    "    >>> model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "    >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "    >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    >>> # Generate\n",
    "    >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n",
    "    >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n",
    "    ```\"\"\"\n",
    "    global guidance_scale, hook_layer_list\n",
    "    \n",
    "    output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "    output_hidden_states = (\n",
    "        output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "    )\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    ########\n",
    "    # base #\n",
    "    ########\n",
    "    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "    outputs = self.model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "        cache_position=cache_position,\n",
    "    )\n",
    "\n",
    "    hidden_states = outputs[0]\n",
    "    if self.config.pretraining_tp > 1:\n",
    "        lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "        logits_o = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "        logits_o = torch.cat(logits_o, dim=-1)\n",
    "    else:\n",
    "        logits_o = self.lm_head(hidden_states)\n",
    "    logits_o = logits_o.float()\n",
    "\n",
    "    #######\n",
    "    # PAG #\n",
    "    #######\n",
    "    # 1. hook (make attn map identity)\n",
    "    handles = []\n",
    "    for layer_name in hook_layer_list:\n",
    "        module = get_module_by_name(self, layer_name)\n",
    "        handle = module.register_forward_hook(perturb_attn_map, with_kwargs=True)\n",
    "        handles.append(handle)\n",
    "\n",
    "    # 2. compute logit \n",
    "    # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "    outputs = self.model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        position_ids=position_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "        cache_position=cache_position,\n",
    "    )\n",
    "\n",
    "    hidden_states = outputs[0]\n",
    "    if self.config.pretraining_tp > 1:\n",
    "        lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "        logits_p = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "        logits_p = torch.cat(logits_p, dim=-1)\n",
    "    else:\n",
    "        logits_p = self.lm_head(hidden_states)\n",
    "    logits_p = logits_p.float()\n",
    "\n",
    "    # remove hook\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    # 3. guidance\n",
    "    logits = logits_o + guidance_scale * (logits_o - logits_p)\n",
    "\n",
    "    loss = None\n",
    "    if not return_dict:\n",
    "        output = (logits,) + outputs[1:]\n",
    "        return (loss,) + output if loss is not None else output\n",
    "\n",
    "    return CausalLMOutputWithPast(\n",
    "        loss=loss,\n",
    "        logits=logits,\n",
    "        past_key_values=outputs.past_key_values,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions,\n",
    "    )\n",
    "\n",
    "\n",
    "# monkey patch (ref;  https://discuss.pytorch.org/t/monkey-patching-the-forward-pass-of-an-nn-module/176095)\n",
    "# old_forward = model.forward\n",
    "import types\n",
    "pipe.model.forward = types.MethodType(forward, pipe.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 0.0 ###\n",
      "264 + 117 117\n",
      "2012-03-23 2012-03-24 2012 2013 2014 2014 2015 2018 2016 2015  2015 2016 2014 2013 2014  2018\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 1.0 ###\n",
      "264 + 117 = 0 + 0. In the case of the latter, the number of zeros is 1. The number of zeros in the first column is 1 + 2 and the number of ones is 1 in the second column. The number of zeros in the second is zero columns is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 2.0 ###\n",
      "264 + 117 = 0 + 0. In the 2010s, she has been married to her husband's partner in life.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 3.0 ###\n",
      "264 + 117 =\n",
      "+  +  +  + ------>   +    + => => => replace     =\n",
      "+  +                  =>      =>    replace        =>    replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace replace\n",
      "\n",
      "\n",
      "### 4.0 ###\n",
      "264 + 117 =\n",
      "+  + = volume times volume :\n",
      "+ + + strength + + stronger salt stronger sugar stronger spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices spices\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hook_layer_list = [\n",
    "    # 'model.layers.0.self_attn',\n",
    "    # 'model.layers.1.self_attn',\n",
    "    # 'model.layers.2.self_attn',\n",
    "    # 'model.layers.3.self_attn',\n",
    "    # 'model.layers.4.self_attn',\n",
    "    # 'model.layers.5.self_attn',\n",
    "    # 'model.layers.6.self_attn',\n",
    "    # 'model.layers.7.self_attn',\n",
    "    # 'model.layers.8.self_attn',\n",
    "    # 'model.layers.9.self_attn',\n",
    "    # 'model.layers.10.self_attn',\n",
    "    # 'model.layers.11.self_attn',\n",
    "    # 'model.layers.12.self_attn',\n",
    "    'model.layers.13.self_attn',\n",
    "    'model.layers.14.self_attn',\n",
    "    'model.layers.15.self_attn',\n",
    "    'model.layers.16.self_attn',\n",
    "    'model.layers.17.self_attn',\n",
    "    'model.layers.18.self_attn',\n",
    "    'model.layers.19.self_attn',\n",
    "    'model.layers.20.self_attn',\n",
    "    # 'model.layers.21.self_attn',\n",
    "    # 'model.layers.22.self_attn',\n",
    "    # 'model.layers.23.self_attn',\n",
    "    # 'model.layers.24.self_attn',\n",
    "    # 'model.layers.25.self_attn',\n",
    "    # 'model.layers.26.self_attn',\n",
    "    # 'model.layers.27.self_attn',\n",
    "    # 'model.layers.28.self_attn',\n",
    "    # 'model.layers.29.self_attn',\n",
    "    # 'model.layers.30.self_attn',\n",
    "    # 'model.layers.31.self_attn',\n",
    "]\n",
    "import numpy as np\n",
    "\n",
    "guidance_scale_list = np.linspace(0, 4, 5)\n",
    "for guidance_scale in guidance_scale_list:\n",
    "    prompt = \"264 + 117\"\n",
    "    # prompt = \"# python code for adding two numbers \\n\\n def \"\n",
    "    outputs = pipe(prompt, max_length=64, truncation=True)\n",
    "    print()\n",
    "    print(f'### {round(guidance_scale, 2)} ###')\n",
    "    print(outputs[0]['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 0 ###\n",
      "Today is the day when we can all be proud of our country and our values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 1 ###\n",
      "Today is Election Day and the Republican Party is in a state of flux. The party\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 2 ###\n",
      "Today is Election Day and the Republican Party is in a state of flux. The party\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 3 ###\n",
      "Today is Election Day and Donald Trump is the presumptive nominee. What will happen to jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 4 ###\n",
      "Today is Election Day and Donald Trump is the presumptive nominee. Do not underestimate him.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 5 ###\n",
      "Today is Election Day and Donald Trump is the presumptive nominee. Do not ever confuse good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 6 ###\n",
      "Today is Election Day and Mitt and I both lost because of it————\" exclaimed Mitt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 7 ###\n",
      "Today is Election Day and Mitt and I both lost because people thought I was crazy for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 8 ###\n",
      "Today is Election Day and Mitt and I both lost because people saw me winning because people\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 9 ###\n",
      "Today is Election Day again and again throughout Southern andcentral America voting peacefully intoulsicular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 10 ###\n",
      "Today is Election Eve and Jill continue her historic opposition to unaccountability in politics today afternoon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 11 ###\n",
      "Today is Election Eve and Jill continue her historic opposition to unaccountability in politics today afternoon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 12 ###\n",
      "Today is Election Season when integrity advocates stockpile stinkiolDOSDOSEmpty(), stinkivia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 13 ###\n",
      "Today is Election Season when integrity advocates stockpile stinkiolDOS executable's withoutifying Unifiedavis\n",
      "\n",
      "### Generated text - guidance_scale 14 ###\n",
      "Today is Election Season when integrity advocates stockpile stinkiolDOS executable's withoutifying Unifiedavis\n"
     ]
    }
   ],
   "source": [
    "hook_layer_list = [\n",
    "    # 'transformer.h.0.attn', \n",
    "    # 'transformer.h.1.attn', \n",
    "    'transformer.h.2.attn', \n",
    "    'transformer.h.3.attn', \n",
    "    'transformer.h.4.attn', \n",
    "    'transformer.h.5.attn', \n",
    "    # 'transformer.h.6.attn', \n",
    "    # 'transformer.h.7.attn', \n",
    "    # 'transformer.h.8.attn', \n",
    "    # 'transformer.h.9.attn', \n",
    "    # 'transformer.h.10.attn', \n",
    "    # 'transformer.h.11.attn', \n",
    "]\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "for guidance_scale in range(15):\n",
    "    guidance_traj = []\n",
    "\n",
    "    inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    # generated result\n",
    "    print()\n",
    "    print(f\"### Generated text - guidance_scale {guidance_scale} ###\")\n",
    "    print(f'{tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 계산 함수\n",
    "# def calculate_perplexity(model_original, model_modified, tokenizer, text, guidance_scale):\n",
    "#     encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "#     attention_mask = torch.ones_like(encoded_text)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs_original = model_original(encoded_text, attention_mask=attention_mask, labels=encoded_text)\n",
    "#         loss_original = outputs_original.loss\n",
    "        \n",
    "#         outputs_modified = model_modified(encoded_text, attention_mask=attention_mask)\n",
    "#         logits_modified = outputs_modified.logits\n",
    "        \n",
    "#         logits = outputs_original.logits + guidance_scale * (outputs_original.logits - logits_modified)\n",
    "#         loss_modified = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), encoded_text.view(-1))\n",
    "        \n",
    "#         perplexity = torch.exp((loss_original + loss_modified) / 2)\n",
    "    \n",
    "#     return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 계산 및 출력\n",
    "# perplexity = calculate_perplexity(model_original, model_modified, tokenizer, generated_text, guidance_scale)\n",
    "# print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
