{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit PAG\n",
    "\n",
    "GenerationMixin.generate 이 좀 어질어질하네.\n",
    "\n",
    "되도록 건드리지 않는 것을 추구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hook\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pag.utils import get_module_by_name\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "def perturb_attn_map(module, input, kwargs, output, ):\n",
    "    layer_past = kwargs['layer_past']\n",
    "    use_cache = kwargs['use_cache']\n",
    "    head_mask = kwargs['head_mask']\n",
    "    attention_mask = kwargs['attention_mask']\n",
    "    output_attentions = kwargs['output_attentions']\n",
    "\n",
    "    hidden_states = input[0]\n",
    "    \n",
    "    query, key, value = module.c_attn(hidden_states).split(module.split_size, dim=2)\n",
    "\n",
    "    query = module._split_heads(query, module.num_heads, module.head_dim)\n",
    "    key = module._split_heads(key, module.num_heads, module.head_dim)\n",
    "    value = module._split_heads(value, module.num_heads, module.head_dim)\n",
    "    \n",
    "    if layer_past is not None:\n",
    "        past_key, past_value = layer_past\n",
    "        key = torch.cat((past_key, key), dim=-2)\n",
    "        value = torch.cat((past_value, value), dim=-2)\n",
    "\n",
    "    if use_cache is True:\n",
    "        present = (key, value)\n",
    "    else:\n",
    "        present = None\n",
    "\n",
    "    # make perturbed attn_mask\n",
    "    l_k, l_q = key.size(2), query.size(2)\n",
    "    attention_mask = torch.zeros(l_k, l_k) + torch.eye(l_k)\n",
    "    attention_mask = (1.0 - attention_mask) * torch.finfo(hidden_states.dtype).min\n",
    "    attention_mask = attention_mask[None, None, :l_q, :l_k] # |b, head, l, l|\n",
    "    \n",
    "    if module.reorder_and_upcast_attn:\n",
    "        attn_output, attn_weights = module._upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n",
    "    else:\n",
    "        attn_output, attn_weights = module._attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "    attn_output = module._merge_heads(attn_output, module.num_heads, module.head_dim)\n",
    "    attn_output = module.c_proj(attn_output)\n",
    "    attn_output = module.resid_dropout(attn_output)\n",
    "\n",
    "    outputs = (attn_output, present)\n",
    "    if output_attentions:\n",
    "        outputs += (attn_weights,)\n",
    "\n",
    "    return outputs  # a, present, (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monkey patch forward path\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    ")\n",
    "\n",
    "def forward(\n",
    "    self,\n",
    "    input_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "    attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    token_type_ids: Optional[torch.LongTensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "    encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = False,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n",
    "    r\"\"\"\n",
    "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "    \"\"\"\n",
    "    global guidance_traj, hook_layer_list, guidance_scale\n",
    "    return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "    ########\n",
    "    # base #\n",
    "    ########\n",
    "    transformer_outputs = self.transformer(\n",
    "        input_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        encoder_attention_mask=encoder_attention_mask,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    hidden_states_o = transformer_outputs[0]\n",
    "\n",
    "    # Set device for model parallelism\n",
    "    if self.model_parallel:\n",
    "        torch.cuda.set_device(self.transformer.first_device)\n",
    "        hidden_states_o = hidden_states_o.to(self.lm_head.weight.device)\n",
    "\n",
    "    lm_logits_o = self.lm_head(hidden_states_o)\n",
    "\n",
    "    #######\n",
    "    # PAG #\n",
    "    #######\n",
    "    # 1. hook (make attn map identity)\n",
    "    handles = []\n",
    "    for layer_name in hook_layer_list:\n",
    "        module = get_module_by_name(self, layer_name)\n",
    "        handle = module.register_forward_hook(perturb_attn_map, with_kwargs=True)\n",
    "        handles.append(handle)\n",
    "\n",
    "    # 2. compute logit \n",
    "    transformer_outputs_p = self.transformer(\n",
    "        input_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        encoder_attention_mask=encoder_attention_mask,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    hidden_states_p = transformer_outputs_p[0]\n",
    "\n",
    "    if self.model_parallel:\n",
    "        torch.cuda.set_device(self.transformer.first_device)\n",
    "        hidden_states_p = hidden_states_p.to(self.lm_head.weight.device)\n",
    "\n",
    "    lm_logits_p = self.lm_head(hidden_states_p)\n",
    "\n",
    "    # 3. remove hook\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    # 4. guidance\n",
    "    lm_logits = lm_logits_o + guidance_scale * (lm_logits_o - lm_logits_p)\n",
    "    guidance_traj.append(lm_logits_o - lm_logits_p)\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (lm_logits,) + transformer_outputs[1:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    return CausalLMOutputWithCrossAttentions(\n",
    "        loss=None,\n",
    "        logits=lm_logits,\n",
    "        past_key_values=transformer_outputs.past_key_values,\n",
    "        hidden_states=transformer_outputs.hidden_states,\n",
    "        attentions=transformer_outputs.attentions,\n",
    "        cross_attentions=transformer_outputs.cross_attentions,\n",
    "    )\n",
    "\n",
    "# monkey patch (ref;  https://discuss.pytorch.org/t/monkey-patching-the-forward-pass-of-an-nn-module/176095)\n",
    "# old_forward = model.forward\n",
    "import types\n",
    "model.forward = types.MethodType(forward, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 0 ###\n",
      "Today is the day when we can all be proud of our country and our values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 1 ###\n",
      "Today is Election Day and the Republican Party is in a state of flux. The party\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 2 ###\n",
      "Today is Election Day and the Republican Party is in a state of flux. The party\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 3 ###\n",
      "Today is Election Day and Donald Trump is the presumptive nominee. What will happen to jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 4 ###\n",
      "Today is Election Day and Donald Trump is the presumptive nominee. Do not underestimate him.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 5 ###\n",
      "Today is Election Day and Donald Trump is the presumptive nominee. Do not ever confuse good\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 6 ###\n",
      "Today is Election Day and Mitt and I both lost because of it————\" exclaimed Mitt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 7 ###\n",
      "Today is Election Day and Mitt and I both lost because people thought I was crazy for\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 8 ###\n",
      "Today is Election Day and Mitt and I both lost because people saw me winning because people\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 9 ###\n",
      "Today is Election Day again and again throughout Southern andcentral America voting peacefully intoulsicular\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 10 ###\n",
      "Today is Election Eve and Jill continue her historic opposition to unaccountability in politics today afternoon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 11 ###\n",
      "Today is Election Eve and Jill continue her historic opposition to unaccountability in politics today afternoon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 12 ###\n",
      "Today is Election Season when integrity advocates stockpile stinkiolDOSDOSEmpty(), stinkivia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Generated text - guidance_scale 13 ###\n",
      "Today is Election Season when integrity advocates stockpile stinkiolDOS executable's withoutifying Unifiedavis\n",
      "\n",
      "### Generated text - guidance_scale 14 ###\n",
      "Today is Election Season when integrity advocates stockpile stinkiolDOS executable's withoutifying Unifiedavis\n"
     ]
    }
   ],
   "source": [
    "hook_layer_list = [\n",
    "    # 'transformer.h.0.attn', \n",
    "    # 'transformer.h.1.attn', \n",
    "    'transformer.h.2.attn', \n",
    "    'transformer.h.3.attn', \n",
    "    'transformer.h.4.attn', \n",
    "    'transformer.h.5.attn', \n",
    "    # 'transformer.h.6.attn', \n",
    "    # 'transformer.h.7.attn', \n",
    "    # 'transformer.h.8.attn', \n",
    "    # 'transformer.h.9.attn', \n",
    "    # 'transformer.h.10.attn', \n",
    "    # 'transformer.h.11.attn', \n",
    "]\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "for guidance_scale in range(15):\n",
    "    guidance_traj = []\n",
    "\n",
    "    inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=15, return_dict_in_generate=True, output_scores=True)\n",
    "\n",
    "    # generated result\n",
    "    print()\n",
    "    print(f\"### Generated text - guidance_scale {guidance_scale} ###\")\n",
    "    print(f'{tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 계산 함수\n",
    "# def calculate_perplexity(model_original, model_modified, tokenizer, text, guidance_scale):\n",
    "#     encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "#     attention_mask = torch.ones_like(encoded_text)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         outputs_original = model_original(encoded_text, attention_mask=attention_mask, labels=encoded_text)\n",
    "#         loss_original = outputs_original.loss\n",
    "        \n",
    "#         outputs_modified = model_modified(encoded_text, attention_mask=attention_mask)\n",
    "#         logits_modified = outputs_modified.logits\n",
    "        \n",
    "#         logits = outputs_original.logits + guidance_scale * (outputs_original.logits - logits_modified)\n",
    "#         loss_modified = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), encoded_text.view(-1))\n",
    "        \n",
    "#         perplexity = torch.exp((loss_original + loss_modified) / 2)\n",
    "    \n",
    "#     return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 계산 및 출력\n",
    "# perplexity = calculate_perplexity(model_original, model_modified, tokenizer, generated_text, guidance_scale)\n",
    "# print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
