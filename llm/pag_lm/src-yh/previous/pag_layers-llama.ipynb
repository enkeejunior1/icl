{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logit PAG\n",
    "\n",
    "GenerationMixin.generate 이 좀 어질어질하네.\n",
    "\n",
    "되도록 건드리지 않는 것을 추구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/nsml/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "token = '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'\n",
    "huggingface_hub.login(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:51<00:00, 12.89s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 12s, sys: 43 s, total: 5min 55s\n",
      "Wall time: 1min 24s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hey how are you doing today? I am doing well. I am a little bit tired because I have been working on my homework all day. I am going to go to bed soon. I hope you are doing well. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are having a good day. I hope you are'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.float16}, device_map=\"cuda\"\n",
    ")\n",
    "pipe(\"Hey how are you doing today?\", max_length=256, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hook\n",
    "from pag.utils import get_module_by_name\n",
    "\n",
    "# Adapted from LlamaAttention.forward\n",
    "def perturb_attn_map(module, input, kwargs, output, guidance_scale: float=7.5):\n",
    "    hidden_states = kwargs['hidden_states']\n",
    "    attention_mask = kwargs['attention_mask']\n",
    "    position_ids = kwargs['position_ids']\n",
    "    past_key_value = kwargs['past_key_value']\n",
    "    output_attentions = kwargs['output_attentions']\n",
    "    use_cache = kwargs['use_cache']\n",
    "    cache_position = kwargs['cache_position']\n",
    "\n",
    "    if output_attentions:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "    query_states = module.q_proj(hidden_states)\n",
    "    key_states = module.k_proj(hidden_states)\n",
    "    value_states = module.v_proj(hidden_states)\n",
    "\n",
    "    query_states = query_states.view(bsz, q_len, module.num_heads, module.head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(bsz, q_len, module.num_key_value_heads, module.head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(bsz, q_len, module.num_key_value_heads, module.head_dim).transpose(1, 2)\n",
    "\n",
    "    cos, sin = module.rotary_emb(value_states, position_ids)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "    # In case static cache is used, it is an instance attribute.\n",
    "    past_key_value = getattr(module, \"past_key_value\", past_key_value)\n",
    "\n",
    "    if past_key_value is not None:\n",
    "        # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "        cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "        key_states, value_states = past_key_value.update(key_states, value_states, module.layer_idx, cache_kwargs)\n",
    "\n",
    "    key_states = repeat_kv(key_states, module.num_key_value_groups)\n",
    "    value_states = repeat_kv(value_states, module.num_key_value_groups)\n",
    "\n",
    "    # original attn\n",
    "    k_len = key_states.size(-2)\n",
    "    causal_mask = attention_mask\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = causal_mask[:, :, :, :k_len]\n",
    "\n",
    "    if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "    attn_output_o = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=causal_mask,\n",
    "        dropout_p=module.attention_dropout if module.training else 0.0,\n",
    "        is_causal=causal_mask is None and q_len > 1,\n",
    "    )\n",
    "\n",
    "    attn_output_o = attn_output_o.transpose(1, 2).contiguous()\n",
    "    attn_output_o = attn_output_o.view(bsz, q_len, module.hidden_size)\n",
    "\n",
    "    attn_output_o = module.o_proj(attn_output_o)\n",
    "\n",
    "    # perturbed attn\n",
    "    attention_mask = torch.zeros(k_len, k_len) + torch.eye(k_len)\n",
    "    attention_mask = attention_mask.bool().to(query_states.device)\n",
    "    attention_mask = attention_mask[None, None, :q_len, :k_len] # |b, head, l, l|\n",
    "\n",
    "    if query_states.device.type == \"cuda\" and causal_mask is not None:\n",
    "        query_states = query_states.contiguous()\n",
    "        key_states = key_states.contiguous()\n",
    "        value_states = value_states.contiguous()\n",
    "\n",
    "    attn_output_p = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=attention_mask,\n",
    "        dropout_p=module.attention_dropout if module.training else 0.0,\n",
    "        is_causal=causal_mask is None and q_len > 1,\n",
    "    )\n",
    "\n",
    "    attn_output_p = attn_output_p.transpose(1, 2).contiguous()\n",
    "    attn_output_p = attn_output_p.view(bsz, q_len, module.hidden_size)\n",
    "\n",
    "    attn_output_p = module.o_proj(attn_output_p)\n",
    "\n",
    "    # guidance\n",
    "    attn_output = attn_output_o + guidance_scale * (attn_output_o - attn_output_p)\n",
    "\n",
    "    return attn_output, None, past_key_value\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`, *optional*):\n",
    "            Deprecated and unused.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 0.0 ###\n",
      "# python code for adding two numbers \n",
      "\n",
      " def  add (a,b): \n",
      "     return  a + b \n",
      "\n",
      "# calling the function \n",
      "print (add(10,20)) \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 0.25 ###\n",
      "# python code for adding two numbers \n",
      "\n",
      " def  add (a,b):\n",
      "     return a+b\n",
      "\n",
      " print (add(10,20))\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### 0.5 ###\n",
      "# python code for adding two numbers \n",
      "\n",
      " def  add (a,b):\n",
      "     return a+b\n",
      "\n",
      "# python code for subtracting two numbers \n",
      "\n",
      " def  subtract (a,b):\n",
      "     return a-b\n",
      "\n",
      "# python code for multiplying two numbers \n",
      "\n",
      " def  multiply (a,b):\n",
      "     return a*b\n",
      "\n",
      "# python code\n",
      "\n",
      "\n",
      "### 0.75 ###\n",
      "# python code for adding two numbers \n",
      "\n",
      " def \n",
      "\n",
      "\n",
      "### 1.0 ###\n",
      "# python code for adding two numbers \n",
      "\n",
      " def 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from functools import partial\n",
    "\n",
    "hook_layer_list = [\n",
    "    # 'model.layers.0.self_attn',\n",
    "    # 'model.layers.1.self_attn',\n",
    "    # 'model.layers.2.self_attn',\n",
    "    # 'model.layers.3.self_attn',\n",
    "    # 'model.layers.4.self_attn',\n",
    "    # 'model.layers.5.self_attn',\n",
    "    # 'model.layers.6.self_attn',\n",
    "    # 'model.layers.7.self_attn',\n",
    "    # 'model.layers.8.self_attn',\n",
    "    # 'model.layers.9.self_attn',\n",
    "    'model.layers.10.self_attn',\n",
    "    'model.layers.11.self_attn',\n",
    "    'model.layers.12.self_attn',\n",
    "    'model.layers.13.self_attn',\n",
    "    'model.layers.14.self_attn',\n",
    "    'model.layers.15.self_attn',\n",
    "    # 'model.layers.16.self_attn',\n",
    "    # 'model.layers.17.self_attn',\n",
    "    # 'model.layers.18.self_attn',\n",
    "    # 'model.layers.19.self_attn',\n",
    "    # 'model.layers.20.self_attn',\n",
    "    # 'model.layers.21.self_attn',\n",
    "    # 'model.layers.22.self_attn',\n",
    "    # 'model.layers.23.self_attn',\n",
    "    # 'model.layers.24.self_attn',\n",
    "    # 'model.layers.25.self_attn',\n",
    "    # 'model.layers.26.self_attn',\n",
    "    # 'model.layers.27.self_attn',\n",
    "    # 'model.layers.28.self_attn',\n",
    "    # 'model.layers.29.self_attn',\n",
    "    # 'model.layers.30.self_attn',\n",
    "    # 'model.layers.31.self_attn',\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "guidance_scale_list = np.linspace(0, 1, 5)\n",
    "for guidance_scale in guidance_scale_list:\n",
    "    # hook (make attn map identity)\n",
    "    handles = []\n",
    "    for layer_name in hook_layer_list:\n",
    "        module = get_module_by_name(pipe.model, layer_name)\n",
    "        perturb_attn_map_fn = partial(perturb_attn_map, guidance_scale=guidance_scale)\n",
    "        handle = module.register_forward_hook(perturb_attn_map_fn, with_kwargs=True)\n",
    "        handles.append(handle)\n",
    "\n",
    "    prompt = \"# python code for adding two numbers \\n\\n def \"\n",
    "    outputs = pipe(prompt, max_length=64, truncation=True)\n",
    "\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "    print()\n",
    "    print(f'### {round(guidance_scale, 2)} ###')\n",
    "    print(outputs[0]['generated_text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# python code for adding two numbers \n",
      "\n",
      " def 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n",
      " 1.0.0\n"
     ]
    }
   ],
   "source": [
    "print(outputs[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
