{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# GPT-2 모델과 토크나이저 로드\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 원본 모델 로드\n",
    "model_original = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Attention map 수정을 위한 모델 복사\n",
    "model_modified = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Attention map을 수정할 layer 번호 설정\n",
    "modify_layers = [2, 4]\n",
    "\n",
    "# Attention map을 identity matrix로 설정하는 함수\n",
    "def modify_attention(module, input, output):\n",
    "    # module이 MultiheadAttention인 경우에만 수정\n",
    "    if isinstance(module, torch.nn.MultiheadAttention):\n",
    "        # layer 번호 가져오기\n",
    "        layer_num = int(module.layer_num)\n",
    "        # 수정할 layer인 경우\n",
    "        if layer_num in modify_layers:\n",
    "            attention_probs = output[1]\n",
    "            batch_size, num_heads, seq_length, _ = attention_probs.shape\n",
    "            print(f\"Layer {layer_num} - Attention shape: {attention_probs.shape}\")\n",
    "            # Attention shape 확인\n",
    "            assert attention_probs.shape[-1] == seq_length, f\"Attention shape mismatch: {attention_probs.shape}\"\n",
    "            # Attention map을 identity matrix로 설정\n",
    "            attention_probs.data = torch.eye(seq_length).expand(batch_size, num_heads, -1, -1)\n",
    "    return output\n",
    "\n",
    "# Hook 등록 함수\n",
    "def register_hooks(model):\n",
    "    # 모든 module에 대해 iterate\n",
    "    for name, module in model.named_modules():\n",
    "        # module이 MultiheadAttention인 경우 hook 등록\n",
    "        if isinstance(module, torch.nn.MultiheadAttention):\n",
    "            layer_num = name.split(\".\")[2]\n",
    "            module.layer_num = layer_num\n",
    "            module.register_forward_hook(modify_attention)\n",
    "\n",
    "# 수정된 모델에 hook 등록\n",
    "register_hooks(model_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성 함수\n",
    "def generate_text(input_text, guidance_scale):\n",
    "    # 입력 텍스트 인코딩\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    # Attention map 수정 후 logit_p 계산\n",
    "    with torch.no_grad():\n",
    "        outputs = model_modified(input_ids, attention_mask=attention_mask)\n",
    "        logit_p = outputs.logits\n",
    "        \n",
    "    # 원래 attention으로 logit_o 계산\n",
    "    with torch.no_grad():\n",
    "        outputs = model_original(input_ids, attention_mask=attention_mask)\n",
    "        logit_o = outputs.logits\n",
    "        \n",
    "    # logit 계산\n",
    "    logit = logit_o + guidance_scale * (logit_o - logit_p)\n",
    "    print(f\"logit shape: {logit.shape}\")\n",
    "    print(f\"logit: {logit}\")\n",
    "    \n",
    "    # 텍스트 생성\n",
    "    output = model_original.generate(\n",
    "        input_ids,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        attention_mask=attention_mask,\n",
    "        logits_processor=[\n",
    "            lambda input_ids, scores: scores + guidance_scale * (scores - logit[:, -1, :])\n",
    "        ],\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated text: {tokenizer.decode(output.sequences[0], skip_special_tokens=True)}\")\n",
    "    print(f\"Processed logits: {output.scores[0].shape}\")\n",
    "    print(f\"Processed logits: {output.scores[0]}\")\n",
    "    \n",
    "    return tokenizer.decode(output.sequences[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logit shape: torch.Size([1, 11, 50257])\n",
      "logit: tensor([[[ -35.2362,  -35.3266,  -38.9754,  ...,  -44.4645,  -43.9975,\n",
      "           -36.4580],\n",
      "         [-112.6171, -114.5832, -116.5724,  ..., -119.0128, -118.8059,\n",
      "          -111.6917],\n",
      "         [-116.7137, -117.5931, -123.1624,  ..., -125.6588, -125.2527,\n",
      "          -119.3150],\n",
      "         ...,\n",
      "         [-107.5247, -109.3616, -113.5464,  ..., -115.4737, -118.1396,\n",
      "          -112.0031],\n",
      "         [ -82.1815,  -84.9542,  -93.1059,  ...,  -98.1118,  -98.1967,\n",
      "           -88.8256],\n",
      "         [-147.8117, -146.8510, -149.7308,  ..., -160.3841, -160.5273,\n",
      "          -143.6174]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Hello, how are you? I'm doing well. I intend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend\n",
      "Processed logits: torch.Size([1, 50257])\n",
      "Processed logits: tensor([[-147.8117, -146.8510, -149.7308,  ..., -160.3841, -160.5273,\n",
      "         -143.6174]])\n",
      "Generated Text:\n",
      "Hello, how are you? I'm doing well. I intend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend tremend\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 생성 및 출력\n",
    "input_text = \"Hello, how are you? I'm doing well.\"\n",
    "guidance_scale = 6.0\n",
    "generated_text = generate_text(input_text, guidance_scale)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity 계산 함수\n",
    "def calculate_perplexity(model_original, model_modified, tokenizer, text, guidance_scale):\n",
    "    encoded_text = tokenizer.encode(text, return_tensors='pt')\n",
    "    attention_mask = torch.ones_like(encoded_text)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_original = model_original(encoded_text, attention_mask=attention_mask, labels=encoded_text)\n",
    "        loss_original = outputs_original.loss\n",
    "        \n",
    "        outputs_modified = model_modified(encoded_text, attention_mask=attention_mask)\n",
    "        logits_modified = outputs_modified.logits\n",
    "        \n",
    "        logits = outputs_original.logits + guidance_scale * (outputs_original.logits - logits_modified)\n",
    "        loss_modified = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), encoded_text.view(-1))\n",
    "        \n",
    "        perplexity = torch.exp((loss_original + loss_modified) / 2)\n",
    "    \n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 535.28\n"
     ]
    }
   ],
   "source": [
    "# Perplexity 계산 및 출력\n",
    "perplexity = calculate_perplexity(model_original, model_modified, tokenizer, generated_text, guidance_scale)\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
