{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reproduce figure \n",
    "\n",
    "objective: THE EFFECTS OF PRETRAINING TASK DIVERSITY ON IN-CONTEXT LEARNING OF RIDGE REGRESSION\n",
    "\n",
    "1. dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "Args = {\n",
    "    'dataset' : {\n",
    "        'sigma'     : 0.1,\n",
    "        'num_tasks' : 32,\n",
    "        'dims'      : 1,\n",
    "        'k'         : 16, # incontext sample\n",
    "        'batch_size': 16,\n",
    "    },\n",
    "    'model' : {\n",
    "        'emb_dims'  : 64,\n",
    "    },\n",
    "    'optim' : {\n",
    "        'epochs' : int(1e4),\n",
    "        'lr'     : 3e-5,\n",
    "    }\n",
    "}\n",
    "args = OmegaConf.structured(Args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "from einops import einsum\n",
    "class Dataset:\n",
    "    def __init__(self, args):\n",
    "        assert args.dataset.dims == 1\n",
    "        self.task_vectors = args.dataset.sigma * torch.randn(\n",
    "            args.dataset.num_tasks, args.dataset.dims\n",
    "        )\n",
    "        self.dims = args.dataset.dims\n",
    "        self.k = args.dataset.k\n",
    "\n",
    "        self.eval_task_vector = 5 * args.dataset.sigma * torch.randn(\n",
    "            args.dataset.dims\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # generate toy dataset\n",
    "        x = torch.randn(self.k, self.dims)\n",
    "        y = x @ self.task_vectors[index]\n",
    "        label = y[-1]\n",
    "        y[-1] = 0\n",
    "        \n",
    "        # organize \n",
    "        input = torch.empty(self.k, 2*self.dims)\n",
    "        input[:, :self.dims] = x\n",
    "        input[:, self.dims:] = y[:, None]\n",
    "        return input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.task_vectors)\n",
    "\n",
    "ds = Dataset(args)\n",
    "dl = torch.utils.data.DataLoader(\n",
    "    ds, batch_size=args.dataset.batch_size, pin_memory=True, num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.embs = args.model.emb_dims\n",
    "        self.emb = nn.Linear(2*args.dataset.dims, self.embs, bias=False)\n",
    "        self.attn = Attention(self.embs)\n",
    "        self.out = nn.Linear(self.embs, args.dataset.dims, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = self.emb(input)\n",
    "        h = self.attn(h)\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, embs):\n",
    "        super().__init__()\n",
    "        self.to_q = nn.Linear(embs, embs, bias=False)\n",
    "        self.to_k = nn.Linear(embs, embs, bias=False)\n",
    "        self.to_v = nn.Linear(embs, embs, bias=False)\n",
    "        # self.to_o = nn.Linear(embs, embs, bias=False)\n",
    "\n",
    "    def forward(self, x, is_causal=True):\n",
    "        # x : |batch_size, seq_len, hid_dim|\n",
    "        q = self.to_q(x)\n",
    "        k = self.to_k(x)\n",
    "        v = self.to_v(x)\n",
    "\n",
    "        # attention mask\n",
    "        L, S = q.size(-2), k.size(-2)\n",
    "        attn_bias = torch.zeros(L, S, dtype=q.dtype)\n",
    "        if is_causal:\n",
    "            temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "            attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias = attn_bias.to(q.device, q.dtype)\n",
    "\n",
    "        # attention calc.\n",
    "        attn_weight = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n",
    "        attn_weight += attn_bias\n",
    "        attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        attn_output = attn_weight @ v\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "model = Model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:11<31:41:48, 11.41s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# validation \u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28minput\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28minput\u001b[39m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;28minput\u001b[39m)[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "Cell \u001b[0;32mIn[64], line 7\u001b[0m, in \u001b[0;36meval\u001b[0;34m(ds, num_eval_samples)\u001b[0m\n\u001b[1;32m      5\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(num_eval_samples, ds\u001b[38;5;241m.\u001b[39mk, ds\u001b[38;5;241m.\u001b[39mdims)\n\u001b[1;32m      6\u001b[0m y \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m@\u001b[39m ds\u001b[38;5;241m.\u001b[39meval_task_vector\n\u001b[0;32m----> 7\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# organize \u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mempty(num_eval_samples, ds\u001b[38;5;241m.\u001b[39mk, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mds\u001b[38;5;241m.\u001b[39mdims)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "device='cuda'\n",
    "\n",
    "def eval(ds, num_eval_samples=512):\n",
    "    x = torch.randn(num_eval_samples, ds.k, ds.dims)\n",
    "    y = x @ ds.eval_task_vector\n",
    "    label = y[:, -1, :]\n",
    "    \n",
    "    # organize \n",
    "    input = torch.empty(num_eval_samples, ds.k, 2*ds.dims)\n",
    "    input[num_eval_samples, :, :ds.dims] = x\n",
    "    input[num_eval_samples, :, ds.dims:] = y[num_eval_samples, :, None]\n",
    "    return input, label\n",
    "\n",
    "# train \n",
    "optim = torch.optim.Adam(model.parameters(), lr=args.optim.lr)\n",
    "model = model.to(device)\n",
    "loss_traj = []\n",
    "val_loss_traj = []\n",
    "for epoch in tqdm(range(args.optim.epochs)):\n",
    "    model.train()\n",
    "    for input, label in dl:\n",
    "        output = model(input.to(device))[:, -1].squeeze()\n",
    "        loss = F.mse_loss(output, label.to(device))\n",
    "        loss.backward()\n",
    "        loss_traj.append(loss.item())\n",
    "\n",
    "    # validation \n",
    "    if epoch % 100 and epoch != 0:\n",
    "        input, label = eval(ds)\n",
    "        input, label = input.to(device), label.to(device)\n",
    "        output = model(input)[:, -1].squeeze()\n",
    "        loss = F.mse_loss(output, label.to(device))\n",
    "        val_loss_traj.append(loss.item())\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
